{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df0a2249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# BiLSTM + fastText Bangla in PyTorch\n",
    "# ================================================\n",
    "import os, re, numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4daddfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "TRAIN_CSV = \"Dataset_60_20_20/train.csv\"\n",
    "VAL_CSV   = \"Dataset_60_20_20/validation.csv\"\n",
    "TEST_CSV  = \"Dataset_60_20_20/test.csv\"\n",
    "\n",
    "EMBED_FILE = \"cc.bn.300.vec\"  # fastText Bangla\n",
    "EMBED_DIM  = 300\n",
    "MAX_LEN    = 300\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS     = 50\n",
    "LR         = 1e-3\n",
    "SEED       = 42\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2f2df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Load data\n",
    "# -----------------------\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df   = pd.read_csv(VAL_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "for d in (train_df, val_df, test_df):\n",
    "    d[\"Summary\"] = d[\"Summary\"].astype(str)\n",
    "    d[\"Genre\"]   = d[\"Genre\"].astype(str)\n",
    "\n",
    "X_train, y_train = train_df[\"Summary\"], train_df[\"Genre\"]\n",
    "X_val,   y_val   = val_df[\"Summary\"],   val_df[\"Genre\"]\n",
    "X_test,  y_test  = test_df[\"Summary\"],  test_df[\"Genre\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ff456d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Adventure', 'Biography and Autobiography', 'Classic Novel', 'Classic Story', 'Contemporary Novel', 'Contemporary Story', 'Cooking, Food and Nutrition', 'History and Tradition', 'Math', 'Mystery', 'Philosophy', 'Politics', 'Religious', 'Sciene Fiction', 'Shishu Kishor', 'Thriller']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Label encoding\n",
    "# -----------------------\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_val_enc   = le.transform(y_val)\n",
    "y_test_enc  = le.transform(y_test)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Classes:\", list(le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfcdd763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 188874\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Tokenizer\n",
    "# -----------------------\n",
    "def tokenize(text):\n",
    "    return re.findall(r'[\\u0980-\\u09FFA-Za-z0-9]+', str(text))\n",
    "\n",
    "# Build vocab from training+val\n",
    "word2idx = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "for txt in pd.concat([X_train, X_val]):\n",
    "    for tok in tokenize(txt):\n",
    "        if tok not in word2idx:\n",
    "            word2idx[tok] = len(word2idx)\n",
    "idx2word = {i:w for w,i in word2idx.items()}\n",
    "vocab_size = len(word2idx)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "def encode(text, max_len=MAX_LEN):\n",
    "    tokens = tokenize(text)\n",
    "    ids = [word2idx.get(t, 1) for t in tokens]\n",
    "    return torch.tensor(ids[:max_len], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b4df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Dataset class\n",
    "# -----------------------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        x = encode(self.texts.iloc[idx])\n",
    "        if self.labels is not None:\n",
    "            y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "def collate_batch(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    xs_pad = pad_sequence(xs, batch_first=True, padding_value=0)\n",
    "    ys = torch.stack(ys)\n",
    "    return xs_pad, ys\n",
    "\n",
    "train_ds = TextDataset(X_train, y_train_enc)\n",
    "val_ds   = TextDataset(X_val, y_val_enc)\n",
    "test_ds  = TextDataset(X_test, y_test_enc)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "test_dl  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5ae5721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings...\n",
      "Embedding coverage: 130530/188874 = 69.11%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Load fastText embeddings\n",
    "# -----------------------\n",
    "def load_embeddings(path, embed_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split(' ')\n",
    "            if len(parts) < embed_dim + 1:  # skip header line\n",
    "                continue\n",
    "            word = parts[0]\n",
    "            try:\n",
    "                vec = np.asarray(parts[1:1+embed_dim], dtype='float32')\n",
    "                embeddings_index[word] = vec\n",
    "            except:\n",
    "                continue\n",
    "    return embeddings_index\n",
    "\n",
    "print(\"Loading embeddings...\")\n",
    "embeddings_index = load_embeddings(EMBED_FILE, EMBED_DIM)\n",
    "embedding_matrix = np.random.normal(0, 0.05, (vocab_size, EMBED_DIM)).astype('float32')\n",
    "hits = 0\n",
    "for word, idx in word2idx.items():\n",
    "    vec = embeddings_index.get(word)\n",
    "    if vec is not None:\n",
    "        embedding_matrix[idx] = vec\n",
    "        hits += 1\n",
    "print(f\"Embedding coverage: {hits}/{vocab_size} = {hits/vocab_size:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d52f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Model\n",
    "# -----------------------\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, embeddings=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        if embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(embeddings))\n",
    "            self.embedding.weight.requires_grad = False  # freeze; set True to fine-tune\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 256)\n",
    "        self.bn = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        _, (h, _) = self.lstm(emb)  # h shape: (2, B, H)\n",
    "        h_cat = torch.cat((h[0], h[1]), dim=1)  # (B, 2H)\n",
    "        out = self.dropout(h_cat)\n",
    "        out = self.fc1(out)\n",
    "        out = self.bn(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc2(out)\n",
    "\n",
    "model = BiLSTMClassifier(vocab_size, EMBED_DIM, 128, num_classes, embedding_matrix).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74045040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 122/122 [00:04<00:00, 27.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 2.0402\n",
      "Epoch 1 Val Acc: 0.4363\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Loss: 1.6875\n",
      "Epoch 2 Val Acc: 0.4423\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Loss: 1.5754\n",
      "Epoch 3 Val Acc: 0.4995\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Loss: 1.4836\n",
      "Epoch 4 Val Acc: 0.4882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Loss: 1.4411\n",
      "Epoch 5 Val Acc: 0.5001\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train Loss: 1.3900\n",
      "Epoch 6 Val Acc: 0.5184\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train Loss: 1.3345\n",
      "Epoch 7 Val Acc: 0.5059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train Loss: 1.3022\n",
      "Epoch 8 Val Acc: 0.5350\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Train Loss: 1.2715\n",
      "Epoch 9 Val Acc: 0.5323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]: 100%|██████████| 122/122 [00:04<00:00, 28.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train Loss: 1.2227\n",
      "Epoch 10 Val Acc: 0.5486\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train Loss: 1.2236\n",
      "Epoch 11 Val Acc: 0.5365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [Train]: 100%|██████████| 122/122 [00:04<00:00, 28.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Train Loss: 1.1648\n",
      "Epoch 12 Val Acc: 0.5490\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [Train]: 100%|██████████| 122/122 [00:04<00:00, 28.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Train Loss: 1.1423\n",
      "Epoch 13 Val Acc: 0.5589\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [Train]: 100%|██████████| 122/122 [00:04<00:00, 28.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Train Loss: 1.1148\n",
      "Epoch 14 Val Acc: 0.5602\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Train Loss: 1.1015\n",
      "Epoch 15 Val Acc: 0.5238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Train Loss: 1.0706\n",
      "Epoch 16 Val Acc: 0.5554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Train Loss: 1.0494\n",
      "Epoch 17 Val Acc: 0.5623\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 [Train]: 100%|██████████| 122/122 [00:04<00:00, 27.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Train Loss: 1.0263\n",
      "Epoch 18 Val Acc: 0.5579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 [Train]: 100%|██████████| 122/122 [00:04<00:00, 28.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Train Loss: 1.0043\n",
      "Epoch 19 Val Acc: 0.5610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Train Loss: 0.9849\n",
      "Epoch 20 Val Acc: 0.5773\n",
      "Saved best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Train Loss: 0.9543\n",
      "Epoch 21 Val Acc: 0.5772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Train Loss: 0.9342\n",
      "Epoch 22 Val Acc: 0.5646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 [Train]: 100%|██████████| 122/122 [00:04<00:00, 29.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Train Loss: 0.9378\n",
      "Epoch 23 Val Acc: 0.5762\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Training loop\n",
    "# -----------------------\n",
    "best_val_acc = 0\n",
    "patience, wait = 3, 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in tqdm(train_dl, desc=f\"Epoch {epoch} [Train]\"):\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch} Train Loss: {total_loss/len(train_dl):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            out = model(xb)\n",
    "            preds = out.argmax(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_true.extend(yb.cpu().numpy())\n",
    "    val_acc = accuracy_score(all_true, all_preds)\n",
    "    print(f\"Epoch {epoch} Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        wait = 0\n",
    "        torch.save(model.state_dict(), \"best_bilstm_fasttext.pt\")\n",
    "        print(\"Saved best model.\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "005190c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== TEST RESULTS ====\n",
      "Accuracy   : 0.5717\n",
      "Macro F1   : 0.4923\n",
      "Weighted F1: 0.5584\n",
      "\n",
      "Classification Report:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                  Adventure     0.4490    0.3099    0.3667        71\n",
      "Biography and Autobiography     0.6953    0.6135    0.6518      1123\n",
      "              Classic Novel     0.1815    0.1960    0.1885       250\n",
      "              Classic Story     0.1538    0.0164    0.0296       122\n",
      "         Contemporary Novel     0.4820    0.8045    0.6028       931\n",
      "         Contemporary Story     0.6434    0.3481    0.4518       451\n",
      "Cooking, Food and Nutrition     0.9259    0.6410    0.7576        39\n",
      "      History and Tradition     0.6947    0.6980    0.6963       639\n",
      "                       Math     0.9630    0.9123    0.9369        57\n",
      "                    Mystery     0.4900    0.3451    0.4050       142\n",
      "                 Philosophy     0.6000    0.4000    0.4800       135\n",
      "                   Politics     0.4444    0.2981    0.3569       161\n",
      "                  Religious     0.7084    0.7993    0.7511       538\n",
      "             Sciene Fiction     0.5033    0.4695    0.4858       164\n",
      "              Shishu Kishor     0.3500    0.3311    0.3403       148\n",
      "                   Thriller     0.4662    0.3151    0.3760       219\n",
      "\n",
      "                   accuracy                         0.5717      5190\n",
      "                  macro avg     0.5469    0.4686    0.4923      5190\n",
      "               weighted avg     0.5762    0.5717    0.5584      5190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Test Evaluation\n",
    "# -----------------------\n",
    "model.load_state_dict(torch.load(\"best_bilstm_fasttext.pt\"))\n",
    "model.eval()\n",
    "all_preds, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        out = model(xb)\n",
    "        preds = out.argmax(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_true.extend(yb.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(all_true, all_preds)\n",
    "f1m = f1_score(all_true, all_preds, average=\"macro\")\n",
    "f1w = f1_score(all_true, all_preds, average=\"weighted\")\n",
    "print(\"\\n==== TEST RESULTS ====\")\n",
    "print(\"Accuracy   :\", f\"{acc:.4f}\")\n",
    "print(\"Macro F1   :\", f\"{f1m:.4f}\")\n",
    "print(\"Weighted F1:\", f\"{f1w:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_true, all_preds, target_names=le.classes_, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414fc4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
