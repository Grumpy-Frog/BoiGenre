{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a95e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# BiLSTM + fastText Bangla (cc.bn.300.vec) for Genre from Summary\n",
    "# ================================================\n",
    "import os, re, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "TRAIN_CSV = \"Dataset_60_20_20/train.csv\"\n",
    "VAL_CSV   = \"Dataset_60_20_20/validation.csv\"\n",
    "TEST_CSV  = \"Dataset_60_20_20/test.csv\"\n",
    "\n",
    "# fastText Bangla vectors (TEXT .vec file)\n",
    "EMBED_FILE = \"cc.bn.300.vec\"  # <-- put the path to your file here\n",
    "EMBED_DIM  = 300              # must match the file\n",
    "\n",
    "MAX_WORDS  = 100_000          # cap vocab size for tokenizer\n",
    "MAX_LEN    = 300              # sequence length (pad/truncate)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS     = 15\n",
    "SEED       = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "843d2e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Load data\n",
    "# -----------------------\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df   = pd.read_csv(VAL_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "for d in (train_df, val_df, test_df):\n",
    "    d[\"Summary\"] = d[\"Summary\"].astype(str)\n",
    "    d[\"Genre\"]   = d[\"Genre\"].astype(str)\n",
    "\n",
    "X_train, y_train = train_df[\"Summary\"], train_df[\"Genre\"]\n",
    "X_val,   y_val   = val_df[\"Summary\"],   val_df[\"Genre\"]\n",
    "X_test,  y_test  = test_df[\"Summary\"],  test_df[\"Genre\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23e6d4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Adventure', 'Biography and Autobiography', 'Classic Novel', 'Classic Story', 'Contemporary Novel', 'Contemporary Story', 'Cooking, Food and Nutrition', 'History and Tradition', 'Math', 'Mystery', 'Philosophy', 'Politics', 'Religious', 'Sciene Fiction', 'Shishu Kishor', 'Thriller']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Labels\n",
    "# -----------------------\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_val_enc   = le.transform(y_val)\n",
    "y_test_enc  = le.transform(y_test)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Classes:\", list(le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38455646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (capped): 100000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Tokenization (Bangla-friendly)\n",
    "# -----------------------\n",
    "# Keep case / avoid stripping Bangla punctuation inconsistently\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, lower=False, filters='')\n",
    "tokenizer.fit_on_texts(X_train.tolist() + X_val.tolist())\n",
    "\n",
    "def to_seq(texts):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(seqs, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "Xtr_seq  = to_seq(X_train)\n",
    "Xval_seq = to_seq(X_val)\n",
    "Xte_seq  = to_seq(X_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = min(MAX_WORDS, len(word_index) + 1)\n",
    "print(\"Vocab size (capped):\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3332d5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: cc.bn.300.vec\n",
      "Embeddings loaded: 1468578\n",
      "Initialized from pre-trained: 61834/100000 = 61.83%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Load fastText .vec embeddings\n",
    "# -----------------------\n",
    "def load_embeddings_txt(path, embed_dim):\n",
    "    \"\"\"\n",
    "    Load text embeddings (word + EMBED_DIM floats per line).\n",
    "    Works for fastText .vec and GloVe-like text files.\n",
    "    Skips header lines that don't have enough fields.\n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split(' ')\n",
    "            # fastText .vec often has a header line \"<vocab> <dim>\"\n",
    "            if len(parts) < embed_dim + 1:\n",
    "                continue\n",
    "            word = parts[0]\n",
    "            try:\n",
    "                vec = np.asarray(parts[1:1+embed_dim], dtype='float32')\n",
    "            except ValueError:\n",
    "                continue\n",
    "            embeddings_index[word] = vec\n",
    "    return embeddings_index\n",
    "\n",
    "print(\"Loading embeddings from:\", EMBED_FILE)\n",
    "embeddings_index = load_embeddings_txt(EMBED_FILE, EMBED_DIM)\n",
    "print(\"Embeddings loaded:\", len(embeddings_index))\n",
    "\n",
    "# Build embedding matrix\n",
    "embedding_matrix = np.random.normal(0.0, 0.05, size=(vocab_size, EMBED_DIM)).astype('float32')\n",
    "hit = 0\n",
    "for w, i in word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    vec = embeddings_index.get(w)\n",
    "    if vec is not None and len(vec) == EMBED_DIM:\n",
    "        embedding_matrix[i] = vec\n",
    "        hit += 1\n",
    "print(f\"Initialized from pre-trained: {hit}/{vocab_size} = {hit/vocab_size:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5135181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# (Optional) Use fastText .bin for OOV coverage via subwords\n",
    "# -----------------------\n",
    "# from gensim.models.fasttext import load_facebook_vectors  # pip install gensim\n",
    "# FT_BIN_FILE = \"cc.bn.300.bin\"\n",
    "# def load_ft_bin_to_matrix(bin_path, word_index, vocab_size, embed_dim):\n",
    "#     ft = load_facebook_vectors(bin_path)\n",
    "#     emb = np.random.normal(0.0, 0.05, size=(vocab_size, embed_dim)).astype('float32')\n",
    "#     hit = 0\n",
    "#     for w, i in word_index.items():\n",
    "#         if i >= vocab_size: \n",
    "#             continue\n",
    "#         try:\n",
    "#             emb[i] = ft.get_vector(w)  # subword composition for OOV too\n",
    "#             hit += 1\n",
    "#         except KeyError:\n",
    "#             pass\n",
    "#     print(f\"Initialized from fastText-bin: {hit}/{vocab_size} = {hit/vocab_size:.2%}\")\n",
    "#     return emb\n",
    "# # To use .bin instead of .vec, uncomment:\n",
    "# # embedding_matrix = load_ft_bin_to_matrix(FT_BIN_FILE, word_index, vocab_size, EMBED_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f918c8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 300)          30000000  \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 256)               439296    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 256)               1024      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                4112      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30510224 (116.39 MB)\n",
      "Trainable params: 509712 (1.94 MB)\n",
      "Non-trainable params: 30000512 (114.44 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Build model\n",
    "# -----------------------\n",
    "def build_model(vocab_size, embed_dim, max_len, embedding_matrix, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embed_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_len,\n",
    "        trainable=False  # set True to fine-tune embeddings\n",
    "    ))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size, EMBED_DIM, MAX_LEN, embedding_matrix, num_classes)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8228916b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "244/244 [==============================] - ETA: 0s - loss: 1.9807 - accuracy: 0.3805\n",
      "Epoch 1: val_accuracy improved from -inf to 0.41148, saving model to best_lstm_fasttext_bn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Installed Softwares\\All kind of IDE or Editors or Programming Language\\Anaconda3\\envs\\nlpthesis\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 [==============================] - 245s 993ms/step - loss: 1.9807 - accuracy: 0.3805 - val_loss: 2.2017 - val_accuracy: 0.4115 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "244/244 [==============================] - ETA: 0s - loss: 1.6187 - accuracy: 0.4728\n",
      "Epoch 2: val_accuracy improved from 0.41148 to 0.50067, saving model to best_lstm_fasttext_bn.h5\n",
      "244/244 [==============================] - 254s 1s/step - loss: 1.6187 - accuracy: 0.4728 - val_loss: 1.6536 - val_accuracy: 0.5007 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "244/244 [==============================] - ETA: 0s - loss: 1.4963 - accuracy: 0.5100\n",
      "Epoch 3: val_accuracy improved from 0.50067 to 0.51570, saving model to best_lstm_fasttext_bn.h5\n",
      "244/244 [==============================] - 262s 1s/step - loss: 1.4963 - accuracy: 0.5100 - val_loss: 1.4700 - val_accuracy: 0.5157 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "173/244 [====================>.........] - ETA: 1:16 - loss: 1.4168 - accuracy: 0.5313"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Train\n",
    "# -----------------------\n",
    "ckpt_path = \"best_lstm_fasttext_bn.h5\"\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1),\n",
    "    ModelCheckpoint(ckpt_path, monitor=\"val_accuracy\", save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    Xtr_seq, y_train_enc,\n",
    "    validation_data=(Xval_seq, y_val_enc),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Evaluate on TEST\n",
    "# -----------------------\n",
    "y_prob = model.predict(Xte_seq, batch_size=BATCH_SIZE)\n",
    "y_pred = y_prob.argmax(axis=1)\n",
    "\n",
    "acc = accuracy_score(y_test_enc, y_pred)\n",
    "print(\"\\nTEST Accuracy:\", f\"{acc:.4f}\")\n",
    "print(\"\\nClassification Report (TEST):\")\n",
    "print(classification_report(y_test_enc, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_enc, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "im = ax.imshow(cm, cmap=\"Blues\")\n",
    "ax.set_title(\"Confusion Matrix - BiLSTM (fastText Bangla)\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "ax.set_xticks(np.arange(num_classes)); ax.set_yticks(np.arange(num_classes))\n",
    "ax.set_xticklabels(le.classes_, rotation=45, ha='right'); ax.set_yticklabels(le.classes_)\n",
    "for i in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        ax.text(j, i, cm[i, j], ha='center', va='center', fontsize=8)\n",
    "fig.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08286c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training curves (optional)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(history.history['accuracy'], label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.title('Accuracy'); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Loss'); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d7044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
