{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1e95f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Train ALL models (no model selection)\n",
    "# - Train on Train\n",
    "# - (Use Val for early stopping when available / report val metrics)\n",
    "# - Test on Test and save classification reports\n",
    "# ==========================================\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# XGBoost (optional)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "555f4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Load data\n",
    "# -----------------------\n",
    "TRAIN_CSV = \"Dataset_60_20_20/train.csv\"\n",
    "VAL_CSV   = \"Dataset_60_20_20/validation.csv\"\n",
    "TEST_CSV  = \"Dataset_60_20_20/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df   = pd.read_csv(VAL_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "for d in (train_df, val_df, test_df):\n",
    "    d[\"Summary\"] = d[\"Summary\"].astype(str)\n",
    "    d[\"Genre\"]   = d[\"Genre\"].astype(str)\n",
    "\n",
    "X_train, y_train = train_df[\"Summary\"], train_df[\"Genre\"]\n",
    "X_val,   y_val   = val_df[\"Summary\"],   val_df[\"Genre\"]\n",
    "X_test,  y_test  = test_df[\"Summary\"],  test_df[\"Genre\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54d25782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "TOKEN_PATTERN = r'[\\u0980-\\u09FFA-Za-z0-9]+'  # Bangla + English tokens\n",
    "MAX_FEATURES = 50000\n",
    "OUT_DIR = \"results_all_models\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3476e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature configs you want to run (edit as needed)\n",
    "tfidf_ngram_sets = {\n",
    "    #\"Unigram\": (1, 1),\n",
    "    #\"Bigram\": (2, 2),\n",
    "    #\"Trigram\": (3, 3),\n",
    "    \"2+3gram\": (2, 3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2757bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Models\n",
    "models_tfidf = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
    "    \"LinearSVM\": LinearSVC(class_weight=\"balanced\"),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "    # GradientBoosting in sklearn can do early stopping via n_iter_no_change + validation_fraction\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42, n_iter_no_change=5, validation_fraction=0.2),\n",
    "}\n",
    "if HAS_XGB:\n",
    "    # We'll add early_stopping via eval_set at fit() time\n",
    "    models_tfidf[\"XGBoost\"] = XGBClassifier(\n",
    "        eval_metric=\"mlogloss\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\"  # faster if GPU not used\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c694f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6daaad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def save_report(prefix, y_true, y_pred, labels=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    f1_weighted = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    rep = classification_report(y_true, y_pred, target_names=labels if labels else None, digits=4)\n",
    "\n",
    "    out_txt = os.path.join(OUT_DIR, f\"{prefix}_report.txt\")\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{prefix}\\n\")\n",
    "        f.write(f\"Accuracy   : {acc:.4f}\\n\")\n",
    "        f.write(f\"Macro F1   : {f1_macro:.4f}\\n\")\n",
    "        f.write(f\"Weighted F1: {f1_weighted:.4f}\\n\\n\")\n",
    "        f.write(rep)\n",
    "    return acc, f1_macro, f1_weighted, out_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd04e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary_rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07e5f325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression-2+3gram\n",
      "LinearSVM-2+3gram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Installed Softwares\\All kind of IDE or Editors or Programming Language\\Anaconda3\\envs\\nlpthesis\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest-2+3gram\n",
      "DecisionTree-2+3gram\n",
      "GradientBoosting-2+3gram\n",
      "XGBoost-2+3gram\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15], got ['Adventure' 'Biography and Autobiography' 'Classic Novel' 'Classic Story'\n 'Contemporary Novel' 'Contemporary Story' 'Cooking, Food and Nutrition'\n 'History and Tradition' 'Math' 'Mystery' 'Philosophy' 'Politics'\n 'Religious' 'Sciene Fiction' 'Shishu Kishor' 'Thriller']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m     Xval_tfidf \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(X_val)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Fit xgb with eval_set for ES\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXtr_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXval_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# For GradientBoosting, we already set n_iter_no_change + validation_fraction.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# But sklearn's GB uses an internal split; here we just fit as usual.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     pipe\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[1;32md:\\Installed Softwares\\All kind of IDE or Editors or Programming Language\\Anaconda3\\envs\\nlpthesis\\lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Installed Softwares\\All kind of IDE or Editors or Programming Language\\Anaconda3\\envs\\nlpthesis\\lib\\site-packages\\xgboost\\sklearn.py:1471\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1468\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1469\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1470\u001b[0m ):\n\u001b[1;32m-> 1471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1472\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1473\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1474\u001b[0m     )\n\u001b[0;32m   1476\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15], got ['Adventure' 'Biography and Autobiography' 'Classic Novel' 'Classic Story'\n 'Contemporary Novel' 'Contemporary Story' 'Cooking, Food and Nutrition'\n 'History and Tradition' 'Math' 'Mystery' 'Philosophy' 'Politics'\n 'Religious' 'Sciene Fiction' 'Shishu Kishor' 'Thriller']"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Run TF-IDF models for each TF-IDF n-gram\n",
    "# -----------------------\n",
    "for vname, ngr in tfidf_ngram_sets.items():\n",
    "    for mname, clf in models_tfidf.items():\n",
    "        name = f\"{mname}-{vname}\"\n",
    "        print(name)\n",
    "        # Build TF-IDF pipeline\n",
    "        pipe = Pipeline([\n",
    "            (\"tfidf\", TfidfVectorizer(token_pattern=TOKEN_PATTERN,\n",
    "                                      ngram_range=ngr,\n",
    "                                      max_features=MAX_FEATURES)),\n",
    "            (\"clf\", clf)\n",
    "        ])\n",
    "\n",
    "        # Train on TRAIN\n",
    "        if mname == \"XGBoost\" and HAS_XGB:\n",
    "            # true early stopping via eval_set=validation\n",
    "            pipe.named_steps[\"tfidf\"].fit(X_train)  # fit vectorizer first\n",
    "            Xtr_tfidf = pipe.named_steps[\"tfidf\"].transform(X_train)\n",
    "            Xval_tfidf = pipe.named_steps[\"tfidf\"].transform(X_val)\n",
    "            # Fit xgb with eval_set for ES\n",
    "            pipe.named_steps[\"clf\"].fit(\n",
    "                Xtr_tfidf, y_train,\n",
    "                eval_set=[(Xval_tfidf, y_val)],\n",
    "                early_stopping_rounds=20,\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            # For GradientBoosting, we already set n_iter_no_change + validation_fraction.\n",
    "            # But sklearn's GB uses an internal split; here we just fit as usual.\n",
    "            pipe.fit(X_train, y_train)\n",
    "\n",
    "        # Validate\n",
    "        y_val_pred = pipe.predict(X_val)\n",
    "        val_acc, val_f1m, val_f1w, _ = save_report(f\"{name}_VAL\", y_val, y_val_pred, labels=sorted(y_train.unique()))\n",
    "\n",
    "        # Test & save report\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "        test_acc, test_f1m, test_f1w, out_path = save_report(f\"{name}_TEST\", y_test, y_test_pred, labels=sorted(y_train.unique()))\n",
    "\n",
    "        summary_rows.append([name, vname, \"TFIDF\", val_acc, val_f1m, val_f1w, test_acc, test_f1m, test_f1w, out_path])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f55583c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Summary (sorted by Test Weighted F1, then Macro F1, then Accuracy) ====\n",
      "                     Model FeatureSet Vectorizer  Val_Acc  Val_MacroF1  Val_WeightedF1  Test_Acc  Test_MacroF1  Test_WeightedF1                                                    ReportPath\n",
      "         LinearSVM-2+3gram    2+3gram      TFIDF 0.512811     0.438515        0.504959  0.532755      0.445642         0.524868          results_all_models\\LinearSVM-2+3gram_TEST_report.txt\n",
      "LogisticRegression-2+3gram    2+3gram      TFIDF 0.494124     0.443307        0.492357  0.510790      0.445101         0.510672 results_all_models\\LogisticRegression-2+3gram_TEST_report.txt\n",
      "      RandomForest-2+3gram    2+3gram      TFIDF 0.444808     0.299444        0.402600  0.462042      0.301903         0.417296       results_all_models\\RandomForest-2+3gram_TEST_report.txt\n",
      "      DecisionTree-2+3gram    2+3gram      TFIDF 0.346947     0.257221        0.333140  0.348940      0.242507         0.333878       results_all_models\\DecisionTree-2+3gram_TEST_report.txt\n",
      "  GradientBoosting-2+3gram    2+3gram      TFIDF 0.333462     0.267331        0.301611  0.337572      0.258116         0.303781   results_all_models\\GradientBoosting-2+3gram_TEST_report.txt\n",
      "\n",
      "Saved summary to: results_all_models\\summary_all_models_2+3gram.csv\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Summary table\n",
    "# -----------------------\n",
    "summary_df = pd.DataFrame(summary_rows, columns=[\n",
    "    \"Model\", \"FeatureSet\", \"Vectorizer\",\n",
    "    \"Val_Acc\", \"Val_MacroF1\", \"Val_WeightedF1\",\n",
    "    \"Test_Acc\", \"Test_MacroF1\", \"Test_WeightedF1\",\n",
    "    \"ReportPath\"\n",
    "]).sort_values([\"Test_WeightedF1\", \"Test_MacroF1\", \"Test_Acc\"], ascending=False)\n",
    "\n",
    "print(\"\\n==== Summary (sorted by Test Weighted F1, then Macro F1, then Accuracy) ====\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary CSV\n",
    "summary_csv = os.path.join(OUT_DIR, \"summary_all_models_2+3gram.csv\")\n",
    "summary_df.to_csv(summary_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nSaved summary to: {summary_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
