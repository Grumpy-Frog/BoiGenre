{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Fine-tune BanglaBERT (PyTorch + HF Transformers)\n",
    "# Train on train.csv, validate on validation.csv, test on test.csv\n",
    "# Fixes device mismatch by moving every batch tensor to DEVICE\n",
    "# ================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Paths & config\n",
    "# -----------------------\n",
    "TRAIN_CSV = \"Dataset_60_20_20/train.csv\"\n",
    "VAL_CSV   = \"Dataset_60_20_20/validation.csv\"\n",
    "TEST_CSV  = \"Dataset_60_20_20/test.csv\"\n",
    "\n",
    "MODEL_NAME = \"sagorsarker/bangla-bert-base\"  # ✅ valid public model id\n",
    "MAX_LEN    = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS     = 50\n",
    "LR         = 2e-5\n",
    "WARMUP_PROP = 0.1\n",
    "PATIENCE    = 2           # early stop on val weighted-F1\n",
    "SEED        = 42\n",
    "OUT_DIR     = \"banglabert_results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reproducibility\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Load data\n",
    "# -----------------------\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df   = pd.read_csv(VAL_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "for d in (train_df, val_df, test_df):\n",
    "    d[\"Summary\"] = d[\"Summary\"].astype(str)\n",
    "    d[\"Genre\"]   = d[\"Genre\"].astype(str)\n",
    "\n",
    "X_train, y_train = train_df[\"Summary\"], train_df[\"Genre\"]\n",
    "X_val,   y_val   = val_df[\"Summary\"],   val_df[\"Genre\"]\n",
    "X_test,  y_test  = test_df[\"Summary\"],  test_df[\"Genre\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Adventure', 'Biography and Autobiography', 'Classic Novel', 'Classic Story', 'Contemporary Novel', 'Contemporary Story', 'Cooking, Food and Nutrition', 'History and Tradition', 'Math', 'Mystery', 'Philosophy', 'Politics', 'Religious', 'Sciene Fiction', 'Shishu Kishor', 'Thriller']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Labels\n",
    "# -----------------------\n",
    "le = LabelEncoder()\n",
    "y_train_ids = le.fit_transform(y_train)\n",
    "y_val_ids   = le.transform(y_val)\n",
    "y_test_ids  = le.transform(y_test)\n",
    "NUM_CLASSES = len(le.classes_)\n",
    "print(\"Classes:\", list(le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Tokenizer\n",
    "# -----------------------\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Dataset (keeps tensors on CPU; we move per-batch to DEVICE)\n",
    "# -----------------------\n",
    "class TextClsDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        enc = tokenize_batch(texts)\n",
    "        self.input_ids = enc[\"input_ids\"]\n",
    "        self.attn_mask = enc[\"attention_mask\"]\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return self.labels.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attn_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_ds = TextClsDataset(X_train, y_train_ids)\n",
    "val_ds   = TextClsDataset(X_val,   y_val_ids)\n",
    "test_ds  = TextClsDataset(X_test,  y_test_ids)\n",
    "\n",
    "pin = (DEVICE == \"cuda\")\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  pin_memory=pin)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, pin_memory=pin)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, pin_memory=pin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\Installed Softwares\\All kind of IDE or Editors or Programming Language\\Anaconda3\\envs\\nlpthesis\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Model, optimizer, scheduler\n",
    "# -----------------------\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=NUM_CLASSES\n",
    ").to(DEVICE)\n",
    "\n",
    "# weight decay for all but bias/LayerNorm\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
    "param_groups = [\n",
    "    {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = AdamW(param_groups, lr=LR)\n",
    "\n",
    "num_training_steps = len(train_dl) * EPOCHS\n",
    "num_warmup_steps = int(WARMUP_PROP * num_training_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Utils\n",
    "# -----------------------\n",
    "def to_device(batch, device):\n",
    "    # move every tensor to DEVICE (fixes CPU/CUDA mismatch)\n",
    "    return {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "        for batch in dataloader:\n",
    "            batch = to_device(batch, DEVICE)\n",
    "            # no need to pass labels for eval unless you want loss\n",
    "            outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_true.extend(batch[\"labels\"].cpu().numpy())\n",
    "    acc = accuracy_score(all_true, all_preds)\n",
    "    f1_macro = f1_score(all_true, all_preds, average=\"macro\")\n",
    "    f1_weighted = f1_score(all_true, all_preds, average=\"weighted\")\n",
    "    return acc, f1_macro, f1_weighted, np.array(all_true), np.array(all_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 974/974 [07:41<00:00,  2.11it/s, loss=2.8027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | TrainLoss 2.8027 | ValAcc 0.0187 | ValF1_w 0.0096\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 974/974 [07:28<00:00,  2.17it/s, loss=2.6729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | TrainLoss 2.6729 | ValAcc 0.2102 | ValF1_w 0.1119\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 974/974 [07:28<00:00,  2.17it/s, loss=2.5012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | TrainLoss 2.5012 | ValAcc 0.2441 | ValF1_w 0.1306\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 974/974 [07:28<00:00,  2.17it/s, loss=2.3907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | TrainLoss 2.3907 | ValAcc 0.2383 | ValF1_w 0.1237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 974/974 [07:28<00:00,  2.17it/s, loss=2.3473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | TrainLoss 2.3473 | ValAcc 0.2828 | ValF1_w 0.1625\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 974/974 [07:28<00:00,  2.17it/s, loss=2.3119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | TrainLoss 2.3119 | ValAcc 0.3063 | ValF1_w 0.1744\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 974/974 [07:28<00:00,  2.17it/s, loss=2.2730]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | TrainLoss 2.2730 | ValAcc 0.3123 | ValF1_w 0.1792\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 974/974 [07:34<00:00,  2.15it/s, loss=2.2422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | TrainLoss 2.2422 | ValAcc 0.3140 | ValF1_w 0.1825\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 974/974 [07:47<00:00,  2.08it/s, loss=2.2170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | TrainLoss 2.2170 | ValAcc 0.3207 | ValF1_w 0.1968\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 974/974 [07:42<00:00,  2.11it/s, loss=2.1854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | TrainLoss 2.1854 | ValAcc 0.3267 | ValF1_w 0.2049\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 974/974 [07:38<00:00,  2.12it/s, loss=2.1616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | TrainLoss 2.1616 | ValAcc 0.3313 | ValF1_w 0.2131\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 974/974 [07:29<00:00,  2.17it/s, loss=2.1424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | TrainLoss 2.1424 | ValAcc 0.3383 | ValF1_w 0.2268\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 974/974 [07:24<00:00,  2.19it/s, loss=2.1183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | TrainLoss 2.1183 | ValAcc 0.3456 | ValF1_w 0.2397\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50:  59%|█████▊    | 572/974 [04:23<03:07,  2.15it/s, loss=1.2375]"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Train (early stop on val Weighted-F1)\n",
    "# -----------------------\n",
    "best_val_f1w = -1.0\n",
    "epochs_no_improve = 0\n",
    "best_path = os.path.join(OUT_DIR, \"banglabert_best.pt\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(train_dl, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    for batch in pbar:\n",
    "        batch = to_device(batch, DEVICE)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "            outputs = model(**batch)  # uses input_ids, attention_mask, labels\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{total_loss/len(pbar):.4f}\")\n",
    "\n",
    "    val_acc, val_f1m, val_f1w, _, _ = evaluate(val_dl)\n",
    "    print(f\"Epoch {epoch} | TrainLoss {total_loss/len(train_dl):.4f} | ValAcc {val_acc:.4f} | ValF1_w {val_f1w:.4f}\")\n",
    "\n",
    "    if val_f1w > best_val_f1w:\n",
    "        best_val_f1w = val_f1w\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(\"  Saved new best model.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Test\n",
    "# -----------------------\n",
    "model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "test_acc, test_f1m, test_f1w, y_true, y_pred = evaluate(test_dl)\n",
    "print(\"\\n==== TEST RESULTS (BanglaBERT) ====\")\n",
    "print(\"Accuracy   :\", f\"{test_acc:.4f}\")\n",
    "print(\"Macro F1   :\", f\"{test_f1m:.4f}\")\n",
    "print(\"Weighted F1:\", f\"{test_f1w:.4f}\")\n",
    "\n",
    "rep = classification_report(y_true, y_pred, target_names=le.classes_, digits=4)\n",
    "print(\"\\nClassification Report:\\n\", rep)\n",
    "\n",
    "# Save report\n",
    "report_path = os.path.join(OUT_DIR, \"banglabert_test_report.txt\")\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"BanglaBERT Test Results\\n\")\n",
    "    f.write(f\"Accuracy   : {test_acc:.4f}\\n\")\n",
    "    f.write(f\"Macro F1   : {test_f1m:.4f}\\n\")\n",
    "    f.write(f\"Weighted F1: {test_f1w:.4f}\\n\\n\")\n",
    "    f.write(rep)\n",
    "print(f\"Saved report to: {report_path}\")\n",
    "\n",
    "# Optional: Confusion matrix PNG\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm, annot=False, cmap=\"Blues\",\n",
    "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title(\"Confusion Matrix - BanglaBERT\")\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    cm_path = os.path.join(OUT_DIR, \"banglabert_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, dpi=200)\n",
    "    plt.show()\n",
    "    print(f\"Saved confusion matrix to: {cm_path}\")\n",
    "except Exception as e:\n",
    "    print(\"Skipped confusion matrix plot:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
