{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd151cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Custom Neural Network (TextCNN + BiLSTM Fusion) in PyTorch\n",
    "# ======================================================\n",
    "import os, re, numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00df2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "TRAIN_CSV = \"Dataset_60_20_20/train.csv\"\n",
    "VAL_CSV   = \"Dataset_60_20_20/validation.csv\"\n",
    "TEST_CSV  = \"Dataset_60_20_20/test.csv\"\n",
    "\n",
    "USE_PRETRAINED = True              # set False to train embeddings from scratch\n",
    "EMBED_FILE     = \"cc.bn.300.vec\"   # fastText Bangla .vec (if USE_PRETRAINED=True)\n",
    "EMBED_DIM      = 300               # 300 for fastText bn\n",
    "MAX_LEN        = 350               # pad/truncate length\n",
    "BATCH_SIZE     = 64\n",
    "EPOCHS         = 50\n",
    "LR             = 1e-3\n",
    "PATIENCE       = 5                 # early stopping patience\n",
    "SEED           = 42\n",
    "DEVICE         = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52a2ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Load data\n",
    "# -----------------------\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df   = pd.read_csv(VAL_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "for d in (train_df, val_df, test_df):\n",
    "    d[\"Summary\"] = d[\"Summary\"].astype(str)\n",
    "    d[\"Genre\"]   = d[\"Genre\"].astype(str)\n",
    "\n",
    "X_train, y_train = train_df[\"Summary\"], train_df[\"Genre\"]\n",
    "X_val,   y_val   = val_df[\"Summary\"],   val_df[\"Genre\"]\n",
    "X_test,  y_test  = test_df[\"Summary\"],  test_df[\"Genre\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "338c6a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Adventure', 'Biography and Autobiography', 'Classic Novel', 'Classic Story', 'Contemporary Novel', 'Contemporary Story', 'Cooking, Food and Nutrition', 'History and Tradition', 'Math', 'Mystery', 'Philosophy', 'Politics', 'Religious', 'Sciene Fiction', 'Shishu Kishor', 'Thriller']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Label encoding\n",
    "# -----------------------\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_val_enc   = le.transform(y_val)\n",
    "y_test_enc  = le.transform(y_test)\n",
    "NUM_CLASSES = len(le.classes_)\n",
    "print(\"Classes:\", list(le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b9b8fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 188874\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Tokenization & Vocab\n",
    "# -----------------------\n",
    "TOKEN_RE = r'[\\u0980-\\u09FFA-Za-z0-9]+'\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(TOKEN_RE, str(text))\n",
    "\n",
    "# build vocab on train+val\n",
    "word2idx = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "for txt in pd.concat([X_train, X_val], axis=0):\n",
    "    for tok in tokenize(txt):\n",
    "        if tok not in word2idx:\n",
    "            word2idx[tok] = len(word2idx)\n",
    "idx2word = {i:w for w,i in word2idx.items()}\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "print(\"Vocab size:\", VOCAB_SIZE)\n",
    "\n",
    "def encode(text, max_len=MAX_LEN):\n",
    "    ids = [word2idx.get(t, 1) for t in tokenize(text)]\n",
    "    ids = ids[:max_len]\n",
    "    return torch.tensor(ids, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59b4f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Dataset / DataLoader\n",
    "# -----------------------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.labels = labels\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        x = encode(self.texts.iloc[idx])\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "def collate(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    xs_pad = pad_sequence(xs, batch_first=True, padding_value=0)\n",
    "    # clip/pad to MAX_LEN to keep tensors aligned\n",
    "    if xs_pad.size(1) < MAX_LEN:\n",
    "        pad_amt = MAX_LEN - xs_pad.size(1)\n",
    "        xs_pad = F.pad(xs_pad, (0, pad_amt), value=0)\n",
    "    elif xs_pad.size(1) > MAX_LEN:\n",
    "        xs_pad = xs_pad[:, :MAX_LEN]\n",
    "    return xs_pad, torch.stack(ys)\n",
    "\n",
    "train_dl = DataLoader(TextDataset(X_train, y_train_enc), batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate)\n",
    "val_dl   = DataLoader(TextDataset(X_val,   y_val_enc),   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "test_dl  = DataLoader(TextDataset(X_test,  y_test_enc),  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb2bb01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: cc.bn.300.vec\n",
      "Pretrained coverage: 130530/188874 = 69.11%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Load fastText embeddings (.vec)\n",
    "# -----------------------\n",
    "embedding_matrix = None\n",
    "if USE_PRETRAINED:\n",
    "    def load_embeddings_txt(path, embed_dim):\n",
    "        emb = {}\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                parts = line.rstrip().split(' ')\n",
    "                if len(parts) < embed_dim + 1:  # skip header\n",
    "                    continue\n",
    "                w = parts[0]\n",
    "                try:\n",
    "                    v = np.asarray(parts[1:1+embed_dim], dtype='float32')\n",
    "                    emb[w] = v\n",
    "                except:\n",
    "                    continue\n",
    "        return emb\n",
    "\n",
    "    print(\"Loading embeddings from:\", EMBED_FILE)\n",
    "    emb_index = load_embeddings_txt(EMBED_FILE, EMBED_DIM)\n",
    "    embedding_matrix = np.random.normal(0, 0.05, (VOCAB_SIZE, EMBED_DIM)).astype('float32')\n",
    "    # make PAD = 0 vector\n",
    "    embedding_matrix[0] = 0.0\n",
    "    hits = 0\n",
    "    for w, idx in word2idx.items():\n",
    "        vec = emb_index.get(w)\n",
    "        if vec is not None and len(vec) == EMBED_DIM:\n",
    "            embedding_matrix[idx] = vec\n",
    "            hits += 1\n",
    "    print(f\"Pretrained coverage: {hits}/{VOCAB_SIZE} = {hits/VOCAB_SIZE:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31143ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Custom Model: TextCNN + BiLSTM fusion\n",
    "# -----------------------\n",
    "class CustomCNNBiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, embeddings=None,\n",
    "                 cnn_kernel_sizes=(3,4,5), cnn_channels=128, lstm_hidden=128,\n",
    "                 embed_trainable=False, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        if embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(embeddings))\n",
    "        self.embedding.weight.requires_grad = embed_trainable\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=cnn_channels, kernel_size=k)\n",
    "            for k in cnn_kernel_sizes\n",
    "        ])\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_hidden, batch_first=True, bidirectional=True)\n",
    "\n",
    "        fused_dim = cnn_channels * len(cnn_kernel_sizes) + lstm_hidden*2\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(fused_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T)\n",
    "        emb = self.embedding(x)                   # (B, T, E)\n",
    "        # --- CNN branch ---\n",
    "        cnn_in = emb.transpose(1, 2)              # (B, E, T)\n",
    "        cnn_feats = []\n",
    "        for conv in self.convs:\n",
    "            c = torch.relu(conv(cnn_in))          # (B, C, T')\n",
    "            p = torch.max(c, dim=2).values        # global max pool -> (B, C)\n",
    "            cnn_feats.append(p)\n",
    "        cnn_out = torch.cat(cnn_feats, dim=1)     # (B, C*#kernels)\n",
    "\n",
    "        # --- BiLSTM branch ---\n",
    "        lstm_out, (h, _) = self.lstm(emb)         # h: (2, B, H)\n",
    "        lstm_feat = torch.cat((h[0], h[1]), dim=1)  # (B, 2H)\n",
    "\n",
    "        # --- Fuse ---\n",
    "        fused = torch.cat([cnn_out, lstm_feat], dim=1)\n",
    "        fused = self.dropout(fused)\n",
    "        fused = self.fc1(fused)\n",
    "        fused = self.bn1(fused)\n",
    "        fused = torch.relu(fused)\n",
    "        fused = self.dropout(fused)\n",
    "        logits = self.fc2(fused)                  # (B, num_classes)\n",
    "        return logits\n",
    "\n",
    "model = CustomCNNBiLSTM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    embeddings=embedding_matrix if USE_PRETRAINED else None,\n",
    "    cnn_kernel_sizes=(3,4,5),\n",
    "    cnn_channels=128,\n",
    "    lstm_hidden=128,\n",
    "    embed_trainable=False,   # set True to fine-tune embeddings\n",
    "    dropout=0.3\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "981025a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 244/244 [00:07<00:00, 33.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | TrainLoss 1.8419 | ValAcc 0.5149\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 244/244 [00:07<00:00, 34.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | TrainLoss 1.3275 | ValAcc 0.5710\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 244/244 [00:07<00:00, 34.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | TrainLoss 1.0710 | ValAcc 0.5868\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 244/244 [00:07<00:00, 33.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | TrainLoss 0.8706 | ValAcc 0.6034\n",
      "  Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 244/244 [00:07<00:00, 33.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | TrainLoss 0.6829 | ValAcc 0.5798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]: 100%|██████████| 244/244 [00:07<00:00, 33.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | TrainLoss 0.5352 | ValAcc 0.5802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]: 100%|██████████| 244/244 [00:07<00:00, 33.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | TrainLoss 0.4235 | ValAcc 0.5876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]: 100%|██████████| 244/244 [00:07<00:00, 33.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | TrainLoss 0.3367 | ValAcc 0.5806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]: 100%|██████████| 244/244 [00:07<00:00, 33.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | TrainLoss 0.2857 | ValAcc 0.5797\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Training with Early Stopping (val accuracy)\n",
    "# -----------------------\n",
    "best_val_acc = 0.0\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in tqdm(train_dl, desc=f\"Epoch {epoch} [Train]\"):\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_preds, val_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            preds = logits.argmax(1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_true.extend(yb.cpu().numpy())\n",
    "    val_acc = accuracy_score(val_true, val_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch} | TrainLoss {running_loss/len(train_dl):.4f} | ValAcc {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        wait = 0\n",
    "        torch.save(model.state_dict(), \"best_custom_cnn_bilstm.pt\")\n",
    "        print(\"  Saved new best model.\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b9c9e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== TEST RESULTS (Custom CNN+BiLSTM) ====\n",
      "Accuracy   : 0.6079\n",
      "Macro F1   : 0.5179\n",
      "Weighted F1: 0.5923\n",
      "\n",
      "Classification Report:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                  Adventure     0.4800    0.3380    0.3967        71\n",
      "Biography and Autobiography     0.6669    0.7079    0.6868      1123\n",
      "              Classic Novel     0.3514    0.2600    0.2989       250\n",
      "              Classic Story     0.1429    0.0082    0.0155       122\n",
      "         Contemporary Novel     0.5550    0.7691    0.6448       931\n",
      "         Contemporary Story     0.6225    0.5521    0.5852       451\n",
      "Cooking, Food and Nutrition     0.8250    0.8462    0.8354        39\n",
      "      History and Tradition     0.7032    0.6635    0.6828       639\n",
      "                       Math     0.8226    0.8947    0.8571        57\n",
      "                    Mystery     0.3885    0.4296    0.4080       142\n",
      "                 Philosophy     0.5784    0.4370    0.4979       135\n",
      "                   Politics     0.4507    0.1988    0.2759       161\n",
      "                  Religious     0.7237    0.7937    0.7571       538\n",
      "             Sciene Fiction     0.6136    0.4939    0.5473       164\n",
      "              Shishu Kishor     0.3617    0.3446    0.3529       148\n",
      "                   Thriller     0.5119    0.3927    0.4444       219\n",
      "\n",
      "                   accuracy                         0.6079      5190\n",
      "                  macro avg     0.5499    0.5081    0.5179      5190\n",
      "               weighted avg     0.5926    0.6079    0.5923      5190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Test evaluation\n",
    "# -----------------------\n",
    "model.load_state_dict(torch.load(\"best_custom_cnn_bilstm.pt\"))\n",
    "model.eval()\n",
    "test_preds, test_true = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_true.extend(yb.cpu().numpy())\n",
    "\n",
    "acc  = accuracy_score(test_true, test_preds)\n",
    "f1m  = f1_score(test_true, test_preds, average=\"macro\")\n",
    "f1w  = f1_score(test_true, test_preds, average=\"weighted\")\n",
    "print(\"\\n==== TEST RESULTS (Custom CNN+BiLSTM) ====\")\n",
    "print(\"Accuracy   :\", f\"{acc:.4f}\")\n",
    "print(\"Macro F1   :\", f\"{f1m:.4f}\")\n",
    "print(\"Weighted F1:\", f\"{f1w:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_true, test_preds, target_names=le.classes_, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f282f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
